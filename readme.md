## to implement:

### nn lib
* sigmoid (done)
* softmax (done)
* backward (done)
* test softamx (done)
* mnist is implemented
* solve a bug cause the one hot encoding to have multiple ones at once. (done)
* mnist problem solved with 3 layers deep network where first 2 layers have relu as act function and last one softmax to compute probabilities. loss function is cross entropy loss.
* derivative of softmax in combination with cross entropy loss (done)
* test network  (done)
-----------

* compute statistics to present in github page.

### Very important Observation:
* deeper netowrk needs smaller learning rate in order to converge correctly!